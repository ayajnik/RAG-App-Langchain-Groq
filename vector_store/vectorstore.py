try:
    import os
    import faiss
    import numpy as np
    from typing import List, Any
    import pickle
    from sentence_transformers import SentenceTransformer
except ImportError as e:
    raise ImportError("Required libraries for vector store are not installed. Please install faiss and numpy packages.") from e

class vectorstore:
    def __init__(self, persist_dir: str = "faiss_store", embedding_model: str = "all-MiniLM-L6-v2"):
        self.persist_dir = persist_dir
        self.index = None
        self.metadata = []
        self.model = SentenceTransformer(embedding_model)
        if not os.path.exists(self.persist_dir):
            os.makedirs(self.persist_dir)

    def add_embeddings(self, embeddings: np.ndarray, metadatas: List[Any]):
        dim = embeddings.shape[1]
        if self.index is None:
            self.index = faiss.IndexFlatL2(dim)
        self.index.add(embeddings)
        if metadatas:
            self.metadata.extend(metadatas)
        print(f"Added {embeddings.shape[0]} embeddings to the vector store.")

    def save(self):
        faiss_path = os.path.join(self.persist_dir, "faiss.index")
        meta_path = os.path.join(self.persist_dir, "metadata.pkl")
        faiss.write_index(self.index, faiss_path)
        with open(meta_path, "wb") as f:
            pickle.dump(self.metadata, f)
        print(f"[INFO] Saved Faiss index and metadata to {self.persist_dir}")

    def load(self):
        faiss_path = os.path.join(self.persist_dir, "faiss.index")
        meta_path = os.path.join(self.persist_dir, "metadata.pkl")
        self.index = faiss.read_index(faiss_path)
        with open(meta_path, "rb") as f:
            self.metadata = pickle.load(f)
        print(f"[INFO] Loaded Faiss index and metadata from {self.persist_dir}")


    def build_from_documents(self, chunks, embeddings: np.ndarray):
        metadatas = [{"text":chunk.page_content} for chunk in chunks]
        self.add_embeddings(embeddings, metadatas)
        self.save()
        
    def search(self, query_embedding: np.ndarray, top_k: int = 5):
        D, I = self.index.search(query_embedding, top_k)
        results = []
        for idx, dist in zip(I[0], D[0]):
            meta = self.metadata[idx] if idx < len(self.metadata) else None
            results.append({"index": idx, "distance": dist, "metadata": meta})
        return results
    

